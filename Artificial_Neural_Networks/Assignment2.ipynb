{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "*Due: November 19th, 23:59 Copenhagen (CET) time.* Some general remarks for handing in exercises:\n",
    "- Each exercise comes with context and code from the exercise-set of which it is a part. It is up to you to recycle the right code. If this notebook can be executed from top to bottom on another computer (given the right libraries are installed and data stored) it makes it easier to give points for exercises that were only partially finished for whatever reason\n",
    "- Make sure to answer each sub-exercise\n",
    "- Commenting amply on your results makes it easier to understand that you were on the right track, even if the answer was wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.4**: Did the network finish training? Consider the generated text across epochs.\n",
    "> 1. In the early batches (0-10), the generated text looks very bad. Can you explain why the low diversity generated text contains almost only the symbol \" \" (that is, spaces)?\n",
    "> 2. The high diversity generated text is strange too, but in a different way. Explain how and why (include an explanation of what the diversity function does)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*Answer 5.1.4.1* <br>\n",
    "The low diversity generated text in which the diversity is close to 0 (around 0.2 in the on_epoch_end function) containly almost only the symbol \" \" since the character predictions made by the functions are less variable. In order to generate text, we take the predictions returned by the model and input it into the sample function. From there, the temperature, or diversity, determines whether characters are sampled from the probability vector of predictions (close to 1) or whether we repeatedly predict the character with the highest probability (close to 0). Thus, in the early batches we have almost all spaces since the diversity is low and the model quickly learns that repeatedly selecting the most common character can reduce loss early on. \n",
    "\n",
    "*Answer 5.1.4.2* <br>\n",
    "The high diversity generated text is messed up as well because there is too much diversity in the predictions so the words don't make any sense since the characters are all sampled from the probability vector of predictions. Since the high diversity text contains a temperature closer to 1, the character predictions are sampled directly from the probability vector returned by the model. However, sampling from this prediction probility vector doesn't account for relationships between the characters, i.e., what characters are likely to come after others, so the sequences of letters predicted are not realisitic (although their distributions might be more representative of the probability vector output by the network). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.1.6**: Do the same as above, but for 40 random letters (e.g. smash away on your keyboard) as seed. What happens? Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer 5.1.6* <br>\n",
    "Even when the seed is 40 random letters, the network is still able to quicky learn to generate text that is reminiscent of the pulp fiction screenplay. While we would expect to see more text generated that is similar to the random text input, we see the same types of text that we trained the network on since we have likely overfit the network to that type of text (from the previous fitting of the model to the seeds from the text). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import requests as rq\n",
    "import sys\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import keras\n",
    "import keras.callbacks\n",
    "from keras.callbacks import TensorBoard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elias\\AppData\\Local\\Temp\\ipykernel_2432\\1536041591.py:16: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
      "C:\\Users\\elias\\AppData\\Local\\Temp\\ipykernel_2432\\1536041591.py:17: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60/60 [==============================] - ETA: 0s - loss: 2.2290 - categorical_crossentropy: 2.2290 - accuracy: 0.5420\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"dnasklgnqrgnqerpignerpggerourgnqeroginoe\"\n",
      "dnasklgnqrgnqerpignerpggerourgnqeroginoe                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"dnasklgnqrgnqerpignerpggerourgnqeroginoe\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnasklgnqrgnqerpignerpggerourgnqeroginoe                                                                                                                                                                                                                                                                     T                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           E                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   t                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"dnasklgnqrgnqerpignerpggerourgnqeroginoe\"\n",
      "dnasklgnqrgnqerpignerpggerourgnqeroginoewe re \n",
      "    n                            a                  e   I               r  I             Vi  lItgUN\n",
      "s                                 Auiik YOSet fnl'th heI? te       T      O   Ii?o      EN    f  \n",
      "EpDi[  t   t s c  st'tllar B           Ay      waroI\n",
      "L    \n",
      " Ad   DEr GK O                   Md                      P            t   h tE               Bosrattlegrety      n          V       h           b Ii  i                   h  f ,   tu o          e  e           a  yM.ein \n",
      "\n",
      "       V      JPSGBAR                 S        BtNTZ         M Jia  ASy    J                     thht  ari    Y  o  Pu                  o   5 T                    pe       TTs            ee        hL    i.                       t sh MU      V    b                  e          E     CINCET           W       T   Yhes fove a the  acnescs                                                          UK MY    ua'\"NBRA                                         TME\n",
      "e        KrAAoe, LSTN'L                    V.  e                   b   Bo     a         a      amewswil         W Esm  oflj        tTuwhD\n",
      "        H a.A\n",
      "'           E                 A IA  D                          biM   tN  V      or                    i'.rM Miin      t r?UBkDcu   B   I   ev           I HEERSuNCNU         is        o   Iaadj    an                    o    U   AE     .bYs    tNBns B                                  hor  B                   n   inNaricnnod wenNte\n",
      " o  j    V  H   tart    Bu        n  Y r      b J                 JH            j  Y    cu erseerofr  T                      o                kee   \n",
      "         a                   patd orut \n",
      "       Y            UnC BP TTtl         t                    oU    iotimer'at.\n",
      "\n",
      "\n",
      "   g         IAT M       DA   Ju  wtr          \n",
      "    g       TVArcec,               Jl d MEfITELYaT    i '    dpt      Bo    h           H          s c   –        JUiX\n",
      "Y                NaG JEISe, x    o  d            e  t     L i?Tu                      (  tonean  E  saa be   J              F                  t  PrC   (I(Hhen     (            B       J      S        r    iPnINE IH            N E   U                   'n   DaWeARlt y e –    i          MDe WS t           \n",
      "           U    IVL c              BI DFI\n",
      "  hPro chttobean'  it h fe   J    S    Ap           SMRSU  V     \n",
      "  g  ULE I           A              Al      c      BUTGK         id               WP                                         U                        oE  F  Noo       hk                     o             I CI LO         tp  Foa          h  JN  t  LJe Wh                E      LUeeu'daun                     (otidit. oa gpyA\n",
      "\n",
      "t        Ih     b       A    Es mH. M    hme                 s  oou ecd :r              teSaM I .e       HINO     Vh                     o. Ld GB AYENBCkSER  a          m   MlC      I    IINRLT                     ve      eyAt   R PEheioraofei  f \n",
      "  o   iil)s.            n       NRNitagxlisroder        B e J      IU   a   \n",
      " YP          BNENATe8EpA \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       l             d      fI RImarotgeto              F  Bluni  e              e        oy           yt HAT\n",
      "\n",
      "     odH etin, oild sto          AEO\n",
      "EM            AO          BG                             – klT             c   e      w N      M          a\"IIgr  n   NtTULMCT         e4     TB VAO\n",
      "             i        r f     Se b    .        GAN\n",
      "e        \n",
      "     e    lV'\"ler  \n",
      "    BhmeEw.\n",
      "\n",
      "       t                 i     ihe,sehapetdrit \n",
      "     a– n                                      UWf LLpfi    lfetU\n",
      "\n",
      "  f   BiiFEBywS\n",
      "AIB    oI  N?TNtrp         g\"les, \n",
      "   a                                              tow \n",
      "SONEGL        aI     TfalkDd tn  \n",
      "                 I fa.\n",
      "\n",
      "S      wue       bIMrYs                        s d            e        i                y   Y                N        h      1    WwheNt         T I g  s              T       cu?    HLy a  M       e BcinscinCotp.y, o        NM      Y ,  I0         fran. Re       owe f T  Fb Nudlb  \n",
      " B         E'              IieTYaho    B'   I      sngmsrednd \n",
      " ?O              JteaOmahe o  VUhtcauos\n",
      " \n",
      "       M                        y        l          (     npe tenaT\n",
      "                               B   A       JC  g            e   o oo'    Bg         B                               Auff   w    o  e        G D   tHas, Yl                   W B        IT \n",
      "     ii      MAt pf E   or T\n",
      "            D  \"inJi tu cE)s                I               W          iklene          Ueh  UMaV OJ  WP            o      CX NOWoo  g       S            OE s       AH      wauen                o    R   –   d thidaouytug C:n\n",
      "\n",
      "t   o      P  A               cshbeevwe   U M        OH       th  P     B  h  II FM  f ia              Ih                   i  iHe   a      oU t B  V e tDeuf\n",
      "   m d   the.mwenyfh'fhe f n          I     i       ihi  P  J i l funkefos fenno  u te             lp           PB       M B               s Bi p n  hidr Tho i           oMLFM         W\n",
      "     n B      I         it s      w     A    \n",
      "60/60 [==============================] - 1323s 22s/step - loss: 2.2290 - categorical_crossentropy: 2.2290 - accuracy: 0.5420\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.4644 - categorical_crossentropy: 1.4644 - accuracy: 0.6235\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"dnasklgnqrgnqerpignerpggerourgnqeroginoe\"\n",
      "dnasklgnqrgnqerpignerpggerourgnqeroginoe \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"dnasklgnqrgnqerpignerpggerourgnqeroginoe\"\n",
      "dnasklgnqrgnqerpignerpggerourgnqeroginoe \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"dnasklgnqrgnqerpignerpggerourgnqeroginoe\"\n",
      "dnasklgnqrgnqerpignerpggerourgnqeroginoeice CO \n",
      "                                S JULRD)                                 FTis \n",
      "                                                       sre hes bn \n",
      "                                    brer qunckling to \n",
      "                                    Wo gge \n",
      "                                                                   VI CENCET\n",
      "                                                                    T he e  d in, Dha wom cabe.\n",
      "\n",
      "       The fendig wo With o gat.\n",
      "\n",
      "                     Ly jusne ma s bire un, \n",
      "                                                                      N'ps fus Ticus \n",
      "     X                        PJUCER\n",
      "                                    Hy D RANCE\n",
      "    r oy'w \n",
      "                           Saimins sheus 9ole co the bat wics \n",
      "          VINCENT\n",
      "                           wean             UADCENT\n",
      "                             Whacct. Iar aes \n",
      "                 CIASELDN\n",
      "                                      m Wou dlo yomo the \n",
      "                      PUUMEHE\n",
      "                                                   g                                                                                                                              VINCE\n",
      "                                  BINNE.\n",
      "                                                                                                                                                                                               JUGSWMUS\n",
      "   '                      Yo ard whans, the  oow.\n",
      "\n",
      "                                          Y           Bunce ne the poud mit bindCI \n",
      "                              VINCENqVIAA                                         We tis oredp cancet\n",
      "\n",
      "                                                                                                                              CVANIEH\n",
      "                Thiner aner..E\"\n",
      "                                           no heawuye, in'w worowlke \"neng. bet in tox, pove  he. Machy ou.\n",
      "\n",
      "                                                 MAdMLAN\n",
      "    VIANT\n",
      "                 thps, I'y apr RTSEYS PU ARE \n",
      "                          FAMANHERD                       Joumy, \n",
      "                      MLAS\n",
      "j                                                                                Vack boit woo foow may hleyT\n",
      "                                                           JLNG CaaMSarchap, mmey.\n",
      "\n",
      "                                                                                         S                                             yott bey  PA5NCENT\n",
      "                          Wor  is Wa \n",
      "           CNCENT\n",
      "                           d         MALY\n",
      "SO                                          The go.\n",
      "\n",
      "                                           BINCENG \n",
      "            (ADYos o  f all spans whet KT ton dow a ipt bat, whe CHENCE\n",
      "                                                                                                                                                                                   – cond nus S     Ved, an aowy ano, Boncetion andlat yow \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   tinomanit. Pan'thes hit Vor qunche bi                         VINCE\n",
      "\n",
      "                                .                                                                                                      MVINT\n",
      "             Yu-en. I)\n",
      "                               of dollirr  bis Whe tosthon \"\n",
      "\n",
      "        D co that mrar hang shimhd wo \n",
      "                                                                 Wan'  ANNNEO(                                                                                          in Hhat  na whond PG f.\n",
      "\n",
      "          lowhly mid we \n",
      "                  o ha dein or was, -mangr upf binge ch. Wine thrind, \n",
      "                                                                               in sopldyhitdad, IoVE\n",
      "\n",
      "                                                                       s wAim bfir.\n",
      "\n",
      "             MIF\n",
      "                        BUCENGANS yuy ok bicr ar cewAN\n",
      "\n",
      "           M wh. \n",
      "                                        JULES\n",
      "                    s Mathe \n",
      "                             I'n                       C\" KIOTE\n",
      "                                                                  VUTCENT\n",
      "           Wafik the shin'  wis be dind Tunty.\n",
      "\n",
      "                  onat me roon?\n",
      "\n",
      "           (o)                                                                               uncipn duowa cant yor cesd. Manbi, foub                                                      m  d Angat yow bowes w a s  od ema kea Yound  b                                                                                  mHe's ndus, wa he t awg ao p an pan  I bot in the                                  – gofack p She Gole, Pon \n",
      "                                     bice Whage coritt ce tn'?\n",
      "\n",
      "                                               RAYSMAG\n",
      "Oct sor me she hpo . BUCES\n",
      "                                             (o sutihe cues \n",
      "                       – phe cond \n",
      "                  in mod, thad NANCag rOes h        \n",
      "60/60 [==============================] - 2284s 39s/step - loss: 1.4644 - categorical_crossentropy: 1.4644 - accuracy: 0.6235\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.2436 - categorical_crossentropy: 1.2436 - accuracy: 0.6703\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"dnasklgnqrgnqerpignerpggerourgnqeroginoe\"\n",
      "dnasklgnqrgnqerpignerpggerourgnqeroginoes and the doon the looks a door a door a dored his the with a doom a dood yourd a door a doon the door and the \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"dnasklgnqrgnqerpignerpggerourgnqeroginoe\"\n",
      "dnasklgnqrgnqerpignerpggerourgnqeroginoed spand.\n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                   Malle you's and aus your.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Butch helld the mitht a dall the purds \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Whe lles alle her mered ard and the buckin's lood a doof hill ther herder and fard wo merors live worres alles five \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            INCENT\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"dnasklgnqrgnqerpignerpggerourgnqeroginoe\"\n",
      "dnasklgnqrgnqerpignerpggerourgnqeroginoe.\n",
      "\n",
      "                       hed bagdle berive'r toed deron, \n",
      "                      Butch your-you dowow'suin' HOUNE\n",
      "                                      io youll frove mer on, thes hied \n",
      "                                         Zilltonos sillir for the alld, tob haverslopurs inot urs wil \n",
      "                 Minst lim)fack, surdit.\n",
      "\n",
      "                                                                      Outdfully, hears a dide fut waly.\n",
      "\n",
      "                    tceldy, stalls a doad Mor'll!\n",
      "\n",
      "              Weat tree a dovess ons yourrens belkieffes, thead.\n",
      "\n",
      "                lagd, doind Bued you llugibraldoralla \n",
      "    lies a do! T I's walls wlike paNrs backing, paw seadine)\n",
      "                                 itring.\n",
      "\n",
      "                                dom mes in toldyhe, uruads harka jut Utia neny wave hyad aus ableate or \n",
      "                         VINCENT\n",
      "                         pelr yakrdin'r, he'rs \n",
      "           TH.\n",
      "\n",
      "                 FABIENNE\n",
      "                 Yow fim-\"calds, Rislie.\n",
      "\n",
      "  the bols salle iny welde?\n",
      "\n",
      "               jrats fed I lick, helddplelw the pore-rowprs furd, buary omfus \n",
      "                    bole wime. CZoy, yackin thessend remro \n",
      "                         We'th oper yap\", yound. R2lael.\n",
      "\n",
      "               PARQERS\n",
      "                gert has set? \". RELS\n",
      "                            \"LEUPVING MirOTH\n",
      "\n",
      "                               ViTCEENT\n",
      "                      ut wours LARERApring. It mriall \"Loes Gelit youmn's drunmy will, sor withlarr?\n",
      "\n",
      "           UUNI MAFLARA\n",
      "                Thell gen' a fuokie wit spassi. Julls wankomer laige yourter \n",
      "                       inuls silve. Ind d\n",
      "\n",
      "                          JULES\n",
      "         VincentrE gey stos on towint.\n",
      "\n",
      "                a roudbind..'s \"P-RARSNCE\n",
      "              Way on the folk.\n",
      "\n",
      "                    Nig bodl fures ont insice. sereday und)\n",
      "                           sviis, \n",
      "                VING TOLBWAD\n",
      "                              bered peed youllles jull keard ralds \n",
      "        SHa kanrake junts Woboo your pall.\n",
      "\n",
      "                                    Cakm wi-ssakine wows youyy her y bucking of keodask is sErdiynaed:\n",
      "\n",
      "                                Julrins.\n",
      "\n",
      "                      THacrA\n",
      "             Thein \"urderf abves hamsta figty a \n",
      "                                 Kullifot ales tore\"nt is herdes in the.\n",
      "\n",
      "                 Lors, in and madeser tar, got thur.\n",
      "\n",
      "                  (verlir \n",
      "                                 Jilm, aabe kills iffirt herer?\n",
      "\n",
      "                 gut bedore cayws and.\n",
      "\n",
      "                fis Tan woalringh'lis Theacklalk.\n",
      "\n",
      "               you'se Ite PUNG OOY\n",
      "\n",
      "                      Jules theldon and –ading, wurk yourd Bwat \n",
      "                 T Godmin a anot ane sist atdevzray a foor of fulds.\n",
      "\n",
      "                   Butch howfone sery.\n",
      "\n",
      "                         I'l sitht they hiom. A Welldrea, Nor a \n",
      "                               nomeled your \"Vin e –in.!...)\n",
      "                      You'reegy kinomat in the packilve.\n",
      "\n",
      "                   Woed ares atdomingon'r yourdind andan\" poors yol aarree oumser you \n",
      "                 Vincend apldolk angur was, and sfig in htt yrilds lucking to r Mixmaus, \n",
      "            Ied.\n",
      "\n",
      "        FFACARERDO'S VINCE \n",
      "                                       VINCENT\n",
      "                   FOME S.LLUS\n",
      "\n",
      "                  JULES\n",
      "                  Hoof, hing, Apricl)\n",
      "                            Whent sibackt his I5d!\n",
      "\n",
      "                        Thime the klelt utall you graszed in sheal iomperiok, afre, arrid?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       his wiendat a lfarband.\n",
      "\n",
      "             wea meee yokseler – anddbick, atars \"in.\n",
      "\n",
      "                         Whall whald and tcr oned aing the my it the readm\" and ofwar snodes hist? Ror yous, oullawe noverres, ind?\n",
      "\n",
      "      Buter ard –oUTH. ROARIV MALY – TOMANS\n",
      "\n",
      "                                      ovemelf your hemeleHe's swat to drondysidiy akes frasgs \n",
      "                              fulles acloindin'mo peraling, aike sboeat.\n",
      "\n",
      "                                      I ce perd, houldeldips.\n",
      "\n",
      "                VINCENT\n",
      "                     walk shees.\n",
      "\n",
      "                                 Wilkt or and Jullnth is hilf the a kic the loo?\n",
      "\n",
      "                           JURLMIS\n",
      "                   wobral it you-dien des reverdy's beythale ytut'rnou troed haid andy you min \n",
      "                     MRESRARLRULSIRU\" \n",
      "                    Farcand a ryer.\n",
      "\n",
      "   BULES\n",
      "             Mardomendy.\n",
      "\n",
      "          Mar you loor, Vinces.  erders thin,.\n",
      "\n",
      "                    Vincingst.\n",
      "\n",
      "               Hu kere, \n",
      "                     ked maac nallsnally demes weandy fereatiring oreesidiln.\n",
      "\n",
      "                  andd's bars, you dat the yerist or \n",
      "            comel a doons ruy hefry, your..\n",
      "\n",
      "                                  Whe I sosilive hiet, mere.\"\n",
      "\n",
      "                           Whey, mead?\n",
      "\n",
      "                    (tuyprits orlongacking wrald \"LOLES\n",
      "                    Bulle wair tevig helods Joiven, widt a pors ousis\n",
      "                    MiINCENT\n",
      "  \n",
      "60/60 [==============================] - 1367s 23s/step - loss: 1.2436 - categorical_crossentropy: 1.2436 - accuracy: 0.6703\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.1328 - categorical_crossentropy: 1.1328 - accuracy: 0.6974\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"dnasklgnqrgnqerpignerpggerourgnqeroginoe\"\n",
      "dnasklgnqrgnqerpignerpggerourgnqeroginoes \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"dnasklgnqrgnqerpignerpggerourgnqeroginoe\"\n",
      "dnasklgnqrgnqerpignerpggerourgnqeroginoe \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The stain. And prast want a core the book \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"dnasklgnqrgnqerpignerpggerourgnqeroginoe\"\n",
      "dnasklgnqrgnqerpignerpggerourgnqeroginoe gigron Roks rood.\n",
      "\n",
      "                                         LANCE\n",
      "                    the hagt warkmon fuot a fie bat uppry.\n",
      "\n",
      "                  Madind STALD BUTCHS yo!  Pat youd. AngectT Wes there?\n",
      "\n",
      "                      I'm ghow it?.. TARCELP th the emmpacting freby worowwhaC..Mach pay, \n",
      "                     Yell fon in.) will up stas tarkith \n",
      "               Jreds of Good dpadick Pof Flare Japceans, And \n",
      "                      stit on,  spliled eap out the mpinn. \n",
      "                    Dof wad make to gat yomef capfintopey-a cack, I'l bastever rexind that be nigca rove \n",
      "                              ull frad's Ere ledss the war sho of it and bus a ollust \n",
      "                       JALVI – gos in fod you can and balise a sty car \n",
      "                                                             the fickiarorsiim wall?\n",
      "                       chat way himm aNcha, a veren pwhiy wime Sourd wiyched \n",
      "                                                              I WONY APING) WLOL (POUS MACH OCK / mide we SAan't tig wame poing assed in the \"w-Gns  my is in the stick \n",
      "                               bat ar macbof truck net, red in a Fabing my \n",
      "                (pa pitcerdene)siui, sHand oftle. And a gasilood, I't bemythe DAINDANG RUT Shlies extas rinchdlanty \n",
      "                         Afe dea stes Lakerks whe chald plikipiony \n",
      "                     Jom ers do's whoter we meg byowly \n",
      "                       GPiptin selled th s. brs it \n",
      "                if his litgrorg and of tivin' all, affun Jome \n",
      "           Putmromerung, Vincent a cabpey on a shey tithen \n",
      "               SAVEENCtuced that. ThE bod gown ake 5ious \n",
      "                                                            wis tH\" way me seloing it custe hand a figh is \n",
      "                    got upe ream atted ecan balk way \n",
      "                        to thas spelled Mithe cus er my Mies and fuvers yigat opsain                              Dod on May paytin' n the gut fack tre \n",
      "         aips.\n",
      "\n",
      "                                           JULES\n",
      "                 Dy Butch then store take youy Is'pr I'le sime \n",
      "                    righa ges ay\"\n",
      "\n",
      "                    Zer Mirs.\n",
      "\n",
      "          (to sto tooy orf th sHouns trat git out in they, the the \n",
      "                 We close has that than's theMG-mit reh tha ksit. But with Butch of be verin'tely \n",
      "                     ap of Viccay rabbem wisclus de he pospun.\n",
      "\n",
      "                           S'man'p busting body os she what mam out of \n",
      "                               LEDY ORM TAABIGMAN\n",
      "                             moonghe yougly dook cap the shiods Vuncket youse a sube stait. Now \n",
      "               oo texith, this outh you knnd (to and furenne, mphard Margewe \n",
      "                 the dot tay's har. I'd have mid pars prie \n",
      "     Jumat but the poop.\n",
      "\n",
      "                            Jules slup I apeaded                                                      – gith nemes, I cease tect. Gother. Dow me heancas \n",
      "                He Deat a dach, I'llut dert?\n",
      "\n",
      "                                  moth mest yye \n",
      "            tolagin't. That's unit hers the the \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 I'm a back?S              Houll the shat fime his messelh yuppins nomes gre \n",
      "         TUL H WOCS PAFSANTRHOP – MABING BLAY tiL quile the  preras? .\n",
      "                          ou ambad nck?\n",
      "\n",
      "                                                          Jig.\n",
      "\n",
      "                        he Whit miase to stat in ey.\n",
      "\n",
      "                          "
     ]
    }
   ],
   "source": [
    "response = rq.get(\"http://www.dailyscript.com/scripts/pulp_fiction.html\")\n",
    "text = BeautifulSoup(response.content, \"html.parser\").getText()\n",
    "\n",
    "new_seed = \"dnasklgnqrgnqerpignerpggerourgnqeroginoe\"\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "seqlen = 40\n",
    "step = seqlen\n",
    "sentences = []\n",
    "for i in range(0, len(text) - seqlen - 1, step):\n",
    "    sentences.append(text[i: i + seqlen + 1])\n",
    "\n",
    "x = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, (char_in, char_out) in enumerate(zip(sentence[:-1], sentence[1:])):\n",
    "        x[i, t, char_indices[char_in]] = 1\n",
    "        y[i, t, char_indices[char_out]] = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seqlen, len(chars)), return_sequences=True))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=RMSprop(learning_rate=0.01),\n",
    "    metrics=['categorical_crossentropy', 'accuracy']\n",
    ")\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Helper function to sample an index from a probability array.\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.exp(np.log(preds) / temperature)  # softmax\n",
    "    preds = preds / np.sum(preds)                #\n",
    "    probas = np.random.multinomial(1, preds, 1)  # sample index\n",
    "    return np.argmax(probas)                     #\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    \"\"\"Function invoked at end of each epoch. Prints generated text.\"\"\"\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - seqlen - 1)\n",
    "     \n",
    "    for diversity in [0.2, 0.5, 1.0]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = new_seed\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(5000):\n",
    "            x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)\n",
    "            next_index = sample(preds[0, -1], diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.1**: In your own words, explain what the following function arguments do in\n",
    "the different model loading functions:\n",
    "1. `include_top`: This specifies whether or not to include the fully connected top layer of a pre-trained model. If set to True, the model will include the fully connected layer that was trained on the original dataset, but if set to False, this layer will be excluded, allowing the user to add their own top layer for their specific use case. \n",
    "1. `weights`: This specifies whether to load the weights of a pre-trained model or not. This has a variety of options vailable such as imagenet, None or a path to saved weights.\n",
    "1. `input_shape`: This specifies the shape of the input data that will be used with the pre-trained model.\n",
    "1. `pooling`: This specifies the type of pooling used in the final layer of the model. This can be set to avg, max or None\n",
    "1. `classes`: This specifies the number of classes or outputs for the final output layer of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.2**: Following Jason's example under 'Pre-Trained Model as Classifier'\n",
    "classify [this image](https://images.squarespace-cdn.com/content/v1/58f0ecc029687fbef7b86b03/1583064484458-IM0UKAZIONS6E2CFCDJC/ke17ZwdGBToddI8pDm48kD5ENJpXCfmjfXuRxqpPb-1Zw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpyN2spBBImrH38afc2UL8XBF0s2RHqmX-QW0wG37RpCsIsNysB0CO3b7e86dkNKVNs/Otter+Makes+an+Immediate+U-Turn+Back+to+the+Water.jpg?format=1500w).\n",
    "Print not just the most likely label, but everything that `decode_predictions` returns.\n",
    ">\n",
    "> ***Note***: *The VGG16 model he uses is 500 MB to download, and will take quite long to load and apply.\n",
    "> Rather use one of the smaller models instead ([here](https://keras.io/applications/#documentation-for-individual-models)'s an overview of model sizes), such as DenseNet121.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# example of using a pre-trained model as a classifier\n",
    "from tensorflow.keras.preprocessing.image  import load_img\n",
    "from tensorflow.keras.preprocessing.image  import img_to_array\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.applications.densenet import decode_predictions\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "# load an image from file\n",
    "image = load_img('Otter.jpg', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "# load the model\n",
    "model = DenseNet121()\n",
    "# predict the probability across all output classes\n",
    "yhat = model.predict(image)\n",
    "# convert the probabilities to class labels\n",
    "label = decode_predictions(yhat)\n",
    "# retrieve the results\n",
    "label = [(class_name, prob) for (_, class_name, prob) in label[0]]\n",
    "# print the classification\n",
    "for class_name, prob in label:\n",
    "    print('%s (%.2f%%)' % (class_name, prob*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.2.2:** Now, extract features for each datapoint, using a pre-trained neural network, thus building train and test input matrices `x_train_FE` and `x_test_FE`. Train a logistic regression classifier on the learned features, and report the accuracy on the test data.\n",
    "You should be getting a significantly better performance than when using the raw data. Why is that; what work did the pretrained network do for you to be able to use a linear classifier and get such great performance on a clearly nonlinear problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6.2.2 Code Part 1/2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = np.load('X_cat_vs_dog.npz')['arr_0']\n",
    "Y = np.load('Y_cat_vs_dog.npz')['arr_0']\n",
    "\n",
    "# Split train/test\n",
    "x_train = X[0:500]\n",
    "y_train = Y[0:500]\n",
    "x_test = X[500:]\n",
    "y_test = Y[500:]\n",
    "\n",
    "# Resize images\n",
    "x_train_resized = np.array([resize(img, (224, 224, 3)) for img in x_train])\n",
    "x_test_resized = np.array([resize(img, (224, 224, 3)) for img in x_test])\n",
    "\n",
    "# Preprocess the images for the Densenet model\n",
    "x_train_preprocessed = preprocess_input(x_train_resized)\n",
    "x_test_preprocessed = preprocess_input(x_test_resized)\n",
    "\n",
    "# Extract features using the Densenet model\n",
    "model = DenseNet121(weights='imagenet', include_top=False, pooling='avg')\n",
    "x_train_FE = model.predict(x_train_preprocessed)\n",
    "x_test_FE = model.predict(x_test_preprocessed)\n",
    "\n",
    "# Flatten features\n",
    "x_train_FE = x_train_FE.reshape(x_train_FE.shape[0], -1)\n",
    "x_test_FE = x_test_FE.reshape(x_test_FE.shape[0], -1)\n",
    "\n",
    "# Train logistic regression classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(x_train_FE, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "accuracy = clf.score(x_test_FE, y_test)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer 6.2.2 Writing Part 2/2* <br>\n",
    "The reason why using a pre-trained neural network to extract features results in significantly better performance than using raw data is that the pre-trained network has learned to extract relevant features from images that are useful for classification tasks. By using the output of a hidden layer as features, we are essentially using a compressed representation of the original image that retains important information for classification. This compressed representation is more informative than the raw pixel values and reduces the dimensionality of the data, making it easier for a linear classifier such as logistic regression to separate the different classes. In essence, the pre-trained neural network has done the feature engineering for us, allowing us to focus on building a simple, interpretable classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.1**: What is typically the input and output of an autoencoder? What loss function is typically used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer 7.1.1* <br>\n",
    "The input of an autoencoder is raw data and the output of an autoencoder is a reconstruction of the input based on a compressed version of the input. The loss functions typically used are mean squared error or binary crossentropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.4**: Run the experiment using different values of `latent_dim` (e.g. `[2,16,64,128,512]`) and store the validation loss of the last iteration in the `history` variable for each. Then plot it, my plot looks [like this](https://dhsvendsen.github.io/images/latentdim_vs_reconerror.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim   \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(latent_dim, activation='relu'),\n",
    "            ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Dense(784, activation='sigmoid'),\n",
    "            layers.Reshape((28, 28))\n",
    "            ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "(x_train, _), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "latent_dim_list = [2, 16, 64, 128, 512]\n",
    "history_list = []\n",
    "for dim in latent_dim_list:\n",
    "  autoencoder = Autoencoder(latent_dim = dim)\n",
    "  # autoencoder = layers.Dense(128, activation='relu')(autoencoder)\n",
    "  autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "  history = autoencoder.fit(x_train, x_train,\n",
    "                epochs=10,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n",
    "  history_list.append(history.history['val_loss'][-1])\n",
    "  print(history_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(latent_dim_list, history_list, 'bo-')\n",
    "plt.xlabel('Latent dimension')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.title('Autoencoder validation loss by latent dimension')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.5**: Set the `latent_dim = 2` and describe what happens to the test data - the reconstructed sandal looks off, what do you think happens to it? Then plot the representation of the test data in the latent space, colouring each point according to its class and describe what you see. [Example plot](https://dhsvendsen.github.io/images/two_latent_dims_simple.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer 7.1.5 Writing Part 1/2.* <br>\n",
    "The reconstructed sandal looks very similar to the shoe. When it is shrunk down to the latent space of 2, it sees the sandal and the shoe as similar classes and reconstructs the two similarly. From the plot, we can guess that the colors that are more clearly in their own defined group, such as orange, red, or blue, represent objects that have a more accurate reconstructed image. Whereas the colors that are more scrambled around and messy represent images like the sandal that aren't reconstructed with confident accuracy at this dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 7.1.5 Code Part 2/2\n",
    "autoencoder = Autoencoder(latent_dim = 2)\n",
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "history = autoencoder.fit(x_train, x_train,\n",
    "                epochs=10,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n",
    "\n",
    "# Visualization cell\n",
    "encoded_imgs = autoencoder.encoder(x_test).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i])\n",
    "    plt.title(\"original\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i])\n",
    "    plt.title(\"reconstructed\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(encoded_imgs[:,0],encoded_imgs[:,1],color=[\"C\"+str(i) for i in y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.1**: Explain in your own words how a GAN works. Touch upon:\n",
    "    > * What do the generator and discriminator networks do?\n",
    "    > * What are their respective input and output?\n",
    "    > * Your *iteration_images* should already have some images stored in it. Obeserve them and explain what delicate dance to the two networks engage in during training? What would the accuracy of the discriminator be, faced with a perfect generator? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer 7.2.1* <br>\n",
    "Generative Adversarial Network is a machine learning model that consists of two neural networks (generator and discriminator), which train each other/together to generate more and more accurate predictions. The goal of the generator is to generate a fake sample that fools the discriminatorinto thinking it is actually real. On the flip side, the discriminator has the goal of accurately determining whether the sample presented is real or fake. The generator takes in random noise as its input and outputs a fake sample. The discriminator takes in a sample (either real or fake) and outputs a prediction of whether it is real or fake. The two networks engage in a delicate dance during training where the generator tries to maximize the mistakes or error that the discriminator makes, while the discriminator tries to maximize its accuracy and minimize the errors made. They go back and forth and eventually train each other so that the generator improves at producing samples that are harder to distingiush between real and fake, and the discriminator becomes better at distinguishing between real and fake. Faced with a perfect generator, the accuracy of the discriminator should hypothetically be 50% so it wouldn't actually be able to distiguish between real or fake samples so it should half a 50% chance of predicting correctly each time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.1.1**:  Compared to the autoencoder of last week which mapped the data into a 2-D latent space, but was given enough model complexity to achieve a low reconstruction loss (the one generated in **Ex. 7.1.6**), how are the low dimensional representations of the images distributed in latent space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer 8.1.1* <br>\n",
    "The difference between the autoencoder with low reconstruction loss from last week shows that the cluster of classes in the latent space are more spread out, meaning that they classes are well defined, however, generating something from a point in between the clusters would result in a garbage result from the decoder. On the otherhand the variational autoencoder has the data, clustered, but very closely together where there are no gaps but rather overlaps in the data, meaning it can confuse some classes, but can give you a valid result for generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.1.2**: A VAE has a data reconstruction term (like a regular autoencoder) and a regularizer term. How does the regularization term of the VAE lead to:\n",
    "> - A smoother transition between images in data-space when moving around in latent space\n",
    "> - An effective generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer 8.1.2* <br>\n",
    "The regularization term (KL Divergence) makes it so the results from the encoder are grouped together closer (in a more gausessian distribution) when compared to a traditional autoencoder, making the transitions from classes smoother as they are closer together, and even overlap at times. Now that we know that the datapoints in the the latent space are stochastic (have a probalistic correlation), we can generate beleiveable NEW data by using drawing from this normal distribution and passing it through the decoder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
